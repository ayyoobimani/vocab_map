{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/pppls2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# with initialize(version_base=None, config_path=\"conf\", job_name=\"test_app\"):\n",
    "#     cfg = compose(config_name=\"config\")\n",
    "#     print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# global initialization\n",
    "initialize(version_base=None, config_path=\"conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"config\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_train_text(dataset_file: str, output_file_path: str):\n",
    "    # Load the torch dataset\n",
    "    dataset = load_from_disk(dataset_file)\n",
    "    \n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        # Iterate over the train shard\n",
    "        for instance in dataset['train']:\n",
    "            # Write the text field to the output file\n",
    "            output_file.write(instance['text'] + '\\n')\n",
    "\n",
    "for language in cfg.languages.split(','):\n",
    "\n",
    "    dump_train_text(f'{cfg.datasets_path}/{language}', f'{cfg.raw_data_path}/{language}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### ctd_Latn #################\n",
      "############### pcm_Latn #################\n",
      "############### quc_Latn #################\n",
      "############### wol_Latn #################\n",
      "############### sme_Latn #################\n",
      "############### grc_Grek #################\n",
      "############### ajp_Arab #################\n",
      "############### lzh_Hani #################\n",
      "############### hbo_Hebr #################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//ctd_Latn.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//ctd_Latn --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//ctd_Latn.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ctd_Latn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//ctd_Latn.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 50141 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=6781200\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=188\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 50141 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 50916 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 50141\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 36133\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 36133 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=21619 obj=8.71179 num_tokens=71897 num_tokens/piece=3.32564\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=16439 obj=6.99212 num_tokens=71787 num_tokens/piece=4.36687\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13190 obj=6.91511 num_tokens=73454 num_tokens/piece=5.56892\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13137 obj=6.9026 num_tokens=73495 num_tokens/piece=5.5945\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ctd_Latn.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ctd_Latn.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//pcm_Latn.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//pcm_Latn --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//pcm_Latn.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//pcm_Latn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//pcm_Latn.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (4891 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 185569 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 25 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=55076385\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=989\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 185569 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 205580 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 185569\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 185587\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 185587 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=86772 obj=10.5283 num_tokens=469122 num_tokens/piece=5.40638\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=75032 obj=8.32854 num_tokens=468108 num_tokens/piece=6.23878\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=56268 obj=8.29077 num_tokens=486349 num_tokens/piece=8.64344\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=56245 obj=8.27604 num_tokens=486507 num_tokens/piece=8.64978\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=42183 obj=8.29677 num_tokens=515524 num_tokens/piece=12.2211\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=42181 obj=8.28703 num_tokens=515492 num_tokens/piece=12.221\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=31635 obj=8.32598 num_tokens=549958 num_tokens/piece=17.3845\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=31635 obj=8.31671 num_tokens=549898 num_tokens/piece=17.3826\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=23726 obj=8.37206 num_tokens=586434 num_tokens/piece=24.7169\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=23726 obj=8.36223 num_tokens=586344 num_tokens/piece=24.7131\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17794 obj=8.43752 num_tokens=623638 num_tokens/piece=35.0477\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17794 obj=8.42434 num_tokens=623567 num_tokens/piece=35.0437\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13345 obj=8.5253 num_tokens=660845 num_tokens/piece=49.52\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13345 obj=8.50786 num_tokens=660786 num_tokens/piece=49.5156\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=8.51136 num_tokens=662258 num_tokens/piece=50.1711\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=8.51058 num_tokens=662243 num_tokens/piece=50.1699\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//pcm_Latn.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//pcm_Latn.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//quc_Latn.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//quc_Latn --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//quc_Latn.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//quc_Latn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//quc_Latn.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 179559 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=19382200\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=200\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 179559 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 118683 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 179559\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 136603\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 136603 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=52741 obj=12.2163 num_tokens=380347 num_tokens/piece=7.2116\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=43374 obj=9.87864 num_tokens=384329 num_tokens/piece=8.86082\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=32526 obj=9.82753 num_tokens=395351 num_tokens/piece=12.1549\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=32496 obj=9.80997 num_tokens=396191 num_tokens/piece=12.192\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=24372 obj=9.84522 num_tokens=413355 num_tokens/piece=16.9602\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=24371 obj=9.83577 num_tokens=413294 num_tokens/piece=16.9584\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=18277 obj=9.88768 num_tokens=432099 num_tokens/piece=23.6417\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=18277 obj=9.87681 num_tokens=432040 num_tokens/piece=23.6385\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13707 obj=9.94768 num_tokens=452145 num_tokens/piece=32.9864\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13707 obj=9.93385 num_tokens=452073 num_tokens/piece=32.9812\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=9.94301 num_tokens=454691 num_tokens/piece=34.4463\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=9.94117 num_tokens=454688 num_tokens/piece=34.4461\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//quc_Latn.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//quc_Latn.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//wol_Latn.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//wol_Latn --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//wol_Latn.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//wol_Latn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//wol_Latn.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 145086 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=13417182\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=2830\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 145086 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 290427 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 145086\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 268096\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 268096 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=132135 obj=11.7229 num_tokens=669438 num_tokens/piece=5.06632\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=116531 obj=10.0882 num_tokens=673152 num_tokens/piece=5.77659\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=87352 obj=10.0615 num_tokens=693437 num_tokens/piece=7.93842\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=87204 obj=10.0381 num_tokens=693542 num_tokens/piece=7.9531\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=65400 obj=10.1038 num_tokens=725963 num_tokens/piece=11.1004\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=65396 obj=10.0829 num_tokens=726084 num_tokens/piece=11.1029\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=49047 obj=10.1733 num_tokens=760415 num_tokens/piece=15.5038\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=49047 obj=10.15 num_tokens=760421 num_tokens/piece=15.5039\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=36785 obj=10.2585 num_tokens=796427 num_tokens/piece=21.6509\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=36785 obj=10.2337 num_tokens=796469 num_tokens/piece=21.652\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=27588 obj=10.3605 num_tokens=833124 num_tokens/piece=30.1988\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=27588 obj=10.3329 num_tokens=833178 num_tokens/piece=30.2007\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=20691 obj=10.4783 num_tokens=870640 num_tokens/piece=42.0782\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=20691 obj=10.4469 num_tokens=870725 num_tokens/piece=42.0823\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=15518 obj=10.6133 num_tokens=909467 num_tokens/piece=58.6072\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=15518 obj=10.5762 num_tokens=909554 num_tokens/piece=58.6128\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=10.6797 num_tokens=932215 num_tokens/piece=70.6223\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=10.6566 num_tokens=932314 num_tokens/piece=70.6298\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//wol_Latn.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//wol_Latn.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//sme_Latn.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//sme_Latn --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//sme_Latn.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//sme_Latn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//sme_Latn.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (4473 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 144807 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 4 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=24258031\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=729\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 144807 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 974309 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 144807\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 403469\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 403469 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=234636 obj=14.431 num_tokens=864917 num_tokens/piece=3.68621\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=197437 obj=11.5207 num_tokens=868928 num_tokens/piece=4.40104\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=147956 obj=11.531 num_tokens=909462 num_tokens/piece=6.14684\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=147557 obj=11.4935 num_tokens=910745 num_tokens/piece=6.17216\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=110639 obj=11.6513 num_tokens=972217 num_tokens/piece=8.78729\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=110596 obj=11.612 num_tokens=972187 num_tokens/piece=8.79044\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=82943 obj=11.8241 num_tokens=1038431 num_tokens/piece=12.5198\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=82938 obj=11.7803 num_tokens=1038358 num_tokens/piece=12.5197\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=62200 obj=12.0305 num_tokens=1105423 num_tokens/piece=17.7721\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=62200 obj=11.9837 num_tokens=1105403 num_tokens/piece=17.7718\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=46650 obj=12.2726 num_tokens=1174385 num_tokens/piece=25.1744\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=46649 obj=12.2218 num_tokens=1174378 num_tokens/piece=25.1748\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=34986 obj=12.5455 num_tokens=1243460 num_tokens/piece=35.5416\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=34986 obj=12.4884 num_tokens=1243480 num_tokens/piece=35.5422\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=26239 obj=12.8576 num_tokens=1315205 num_tokens/piece=50.1241\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=26239 obj=12.7921 num_tokens=1315255 num_tokens/piece=50.126\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=19679 obj=13.2104 num_tokens=1388880 num_tokens/piece=70.5768\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=19679 obj=13.1338 num_tokens=1389033 num_tokens/piece=70.5845\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=14758 obj=13.5998 num_tokens=1466678 num_tokens/piece=99.3819\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=14758 obj=13.5134 num_tokens=1466870 num_tokens/piece=99.3949\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=13.6987 num_tokens=1497106 num_tokens/piece=113.417\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=13.6644 num_tokens=1497300 num_tokens/piece=113.432\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//sme_Latn.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//sme_Latn.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//grc_Grek.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//grc_Grek --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//grc_Grek.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//grc_Grek\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//grc_Grek.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (5212 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 94983 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 8 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=11192866\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=297\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 94983 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 318720 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 94983\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 125623\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 125623 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=92709 obj=11.3782 num_tokens=230979 num_tokens/piece=2.49144\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=77725 obj=9.39355 num_tokens=231853 num_tokens/piece=2.98299\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=58269 obj=9.39538 num_tokens=247489 num_tokens/piece=4.24735\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=58225 obj=9.3594 num_tokens=247569 num_tokens/piece=4.25194\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=43665 obj=9.51314 num_tokens=274432 num_tokens/piece=6.28494\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=43662 obj=9.47552 num_tokens=274565 num_tokens/piece=6.28842\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=32745 obj=9.6786 num_tokens=302647 num_tokens/piece=9.24254\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=32745 obj=9.6373 num_tokens=302608 num_tokens/piece=9.24135\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=24558 obj=9.88124 num_tokens=331171 num_tokens/piece=13.4853\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=24558 obj=9.83678 num_tokens=331134 num_tokens/piece=13.4838\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=18418 obj=10.1101 num_tokens=358209 num_tokens/piece=19.4489\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=18418 obj=10.0637 num_tokens=358202 num_tokens/piece=19.4485\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13813 obj=10.3659 num_tokens=383412 num_tokens/piece=27.7573\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13813 obj=10.3169 num_tokens=383375 num_tokens/piece=27.7547\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=10.3611 num_tokens=387023 num_tokens/piece=29.3199\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=10.3539 num_tokens=387030 num_tokens/piece=29.3205\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//grc_Grek.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//grc_Grek.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//ajp_Arab.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//ajp_Arab --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//ajp_Arab.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ajp_Arab\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//ajp_Arab.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (4708 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 81296 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 1 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=6263899\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=861\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 81296 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 213817 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 81296\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 187709\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 187709 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=113172 obj=13.4393 num_tokens=448105 num_tokens/piece=3.9595\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=99040 obj=11.9785 num_tokens=450517 num_tokens/piece=4.54884\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=74260 obj=11.9572 num_tokens=467899 num_tokens/piece=6.30082\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=74129 obj=11.9103 num_tokens=468667 num_tokens/piece=6.32232\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=55593 obj=12.0419 num_tokens=493446 num_tokens/piece=8.87605\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=55590 obj=11.9952 num_tokens=493695 num_tokens/piece=8.881\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=41692 obj=12.1738 num_tokens=519401 num_tokens/piece=12.458\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=41691 obj=12.1212 num_tokens=519479 num_tokens/piece=12.4602\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=31268 obj=12.3393 num_tokens=545424 num_tokens/piece=17.4435\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=31268 obj=12.2785 num_tokens=545413 num_tokens/piece=17.4432\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=23451 obj=12.5329 num_tokens=572156 num_tokens/piece=24.3979\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=23451 obj=12.4627 num_tokens=572161 num_tokens/piece=24.3981\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17588 obj=12.7441 num_tokens=599495 num_tokens/piece=34.0855\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17588 obj=12.6676 num_tokens=599499 num_tokens/piece=34.0857\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=12.9731 num_tokens=625399 num_tokens/piece=47.3787\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=12.8903 num_tokens=625549 num_tokens/piece=47.3901\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ajp_Arab.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//ajp_Arab.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//lzh_Hani.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//lzh_Hani --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//lzh_Hani.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//lzh_Hani\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//lzh_Hani.txt\n",
      "trainer_interface.cc(377) LOG(WARNING) Found too long line (7345 > 4192).\n",
      "trainer_interface.cc(379) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(380) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 84046 sentences\n",
      "trainer_interface.cc(413) LOG(INFO) Skipped 4 too long sentences.\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=4190808\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=11282\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 84046 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 504087 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 84046\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 91029\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 91029 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=339123 obj=235.46 num_tokens=2287147 num_tokens/piece=6.7443\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=312860 obj=223.497 num_tokens=2299327 num_tokens/piece=7.34938\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=234491 obj=225.443 num_tokens=2375152 num_tokens/piece=10.129\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=234043 obj=224.339 num_tokens=2377809 num_tokens/piece=10.1597\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=175501 obj=228.194 num_tokens=2474744 num_tokens/piece=14.101\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=175470 obj=226.838 num_tokens=2477296 num_tokens/piece=14.1181\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=131598 obj=231.492 num_tokens=2574088 num_tokens/piece=19.5602\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=131594 obj=230.156 num_tokens=2575129 num_tokens/piece=19.5687\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=98695 obj=235.346 num_tokens=2671488 num_tokens/piece=27.0681\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=98695 obj=234.11 num_tokens=2672298 num_tokens/piece=27.0763\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=74021 obj=239.623 num_tokens=2772798 num_tokens/piece=37.4596\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=74021 obj=238.475 num_tokens=2773451 num_tokens/piece=37.4684\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=55514 obj=244.157 num_tokens=2875743 num_tokens/piece=51.8021\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=55514 obj=243.037 num_tokens=2876256 num_tokens/piece=51.8114\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=41634 obj=248.899 num_tokens=2981957 num_tokens/piece=71.6231\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=41634 obj=247.801 num_tokens=2982542 num_tokens/piece=71.6372\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=31225 obj=253.884 num_tokens=3096738 num_tokens/piece=99.175\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=31225 obj=252.694 num_tokens=3096991 num_tokens/piece=99.1831\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=23418 obj=259.246 num_tokens=3225537 num_tokens/piece=137.738\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=23418 obj=257.883 num_tokens=3225649 num_tokens/piece=137.742\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17563 obj=265.49 num_tokens=3380304 num_tokens/piece=192.467\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17563 obj=263.817 num_tokens=3381018 num_tokens/piece=192.508\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=274.583 num_tokens=3605511 num_tokens/piece=273.145\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=272.013 num_tokens=3605940 num_tokens/piece=273.177\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//lzh_Hani.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//lzh_Hani.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/mounts/data/proj/ayyoobbig/vocab_map/raw_data//hbo_Hebr.txt --model_prefix=/mounts/data/proj/ayyoobbig/vocab_map/spm_models//hbo_Hebr --vocab_size=12000 --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//hbo_Hebr.txt\n",
      "  input_format: \n",
      "  model_prefix: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//hbo_Hebr\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /mounts/data/proj/ayyoobbig/vocab_map/raw_data//hbo_Hebr.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 94484 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=9143825\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=123\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 94484 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 501029 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 94484\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 241022\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 241022 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=153224 obj=14.7256 num_tokens=510169 num_tokens/piece=3.32956\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=130172 obj=12.5927 num_tokens=512283 num_tokens/piece=3.93543\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=97537 obj=12.5929 num_tokens=535793 num_tokens/piece=5.49323\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=97348 obj=12.525 num_tokens=536391 num_tokens/piece=5.51004\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=73002 obj=12.7768 num_tokens=578788 num_tokens/piece=7.92839\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=72983 obj=12.7056 num_tokens=579168 num_tokens/piece=7.93566\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=54736 obj=13.0217 num_tokens=626069 num_tokens/piece=11.438\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=54732 obj=12.9499 num_tokens=626140 num_tokens/piece=11.4401\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=41049 obj=13.2853 num_tokens=676158 num_tokens/piece=16.472\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=41049 obj=13.2193 num_tokens=676353 num_tokens/piece=16.4767\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=30786 obj=13.5593 num_tokens=724419 num_tokens/piece=23.5308\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=30786 obj=13.4955 num_tokens=724533 num_tokens/piece=23.5345\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=23089 obj=13.8468 num_tokens=767606 num_tokens/piece=33.2455\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=23089 obj=13.7774 num_tokens=767869 num_tokens/piece=33.2569\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=17315 obj=14.1379 num_tokens=807952 num_tokens/piece=46.662\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=17315 obj=14.0647 num_tokens=808113 num_tokens/piece=46.6713\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=13200 obj=14.4227 num_tokens=845116 num_tokens/piece=64.0239\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=13200 obj=14.3476 num_tokens=845197 num_tokens/piece=64.0301\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//hbo_Hebr.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: /mounts/data/proj/ayyoobbig/vocab_map/spm_models//hbo_Hebr.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train spm for each language\n",
    "for language in cfg.languages.split(','):\n",
    "    print(\"###############\", language, \"#################\")\n",
    "    cmd = '--input={} --model_prefix={} --vocab_size={} --model_type=unigram --character_coverage=1.0 --train_extremely_large_corpus=true'.format(\n",
    "        f'{cfg.raw_data_path}/{language}.txt', f'{cfg.spm_models_path}/{language}', cfg.vocab_size)\n",
    "    spm.SentencePieceTrainer.train(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language:  ctd_Latn\n",
      "Add 9577 tokens\n",
      "language:  pcm_Latn\n",
      "Add 4704 tokens\n",
      "language:  quc_Latn\n",
      "Add 9708 tokens\n",
      "language:  wol_Latn\n",
      "Add 5864 tokens\n",
      "language:  sme_Latn\n",
      "Add 8809 tokens\n",
      "language:  grc_Grek\n",
      "Add 10973 tokens\n",
      "language:  ajp_Arab\n",
      "Add 7200 tokens\n",
      "language:  lzh_Hani\n",
      "Add 4251 tokens\n",
      "language:  hbo_Hebr\n",
      "Add 10818 tokens\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_model\n",
    "\n",
    "\n",
    "# Add new tokens of each language to original spm creating a new tokenizer for each lang\n",
    "for language in cfg.languages.split(','):\n",
    "    print('language: ', language)\n",
    "    original_m = sp_model.ModelProto()\n",
    "    original_m.ParseFromString(open(cfg.original_spm, 'rb').read())\n",
    "    new_m = sp_model.ModelProto()\n",
    "    new_m.ParseFromString(open(f'{cfg.spm_models_path}/{language}.model', 'rb').read())\n",
    "\n",
    "    add_cnt = 0 \n",
    "    piece_d = {piece.piece: 1 for piece in original_m.pieces}\n",
    "    for new_piece in new_m.pieces:\n",
    "        if new_piece.piece not in piece_d:\n",
    "            piece_to_add = sp_model.ModelProto().SentencePiece()\n",
    "            # Add token\n",
    "            piece_to_add.piece = new_piece.piece\n",
    "            # Add token log-prob\n",
    "            piece_to_add.score = new_piece.score\n",
    "            original_m.pieces.append(piece_to_add)\n",
    "            add_cnt += 1\n",
    "\n",
    "    print('Add {} tokens'.format(add_cnt))\n",
    "    # logging.info('Add {} tokens'.format(add_cnt))\n",
    "    \n",
    "    new_spm_save_dir = f'{cfg.spm_models_path}/extended_{language}.model'\n",
    "    with open(new_spm_save_dir, 'wb') as f:\n",
    "        f.write(original_m.SerializeToString())\n",
    "    \n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(cfg.HF_tokenizer_model_name)\n",
    "    tokenizer.vocab_file = new_spm_save_dir\n",
    "    tokenizer.sp_model.load(tokenizer.vocab_file)\n",
    "    tokenizer.save_pretrained(f'{cfg.spm_models_path}/HF/extended_{language}/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XLMRoberta model and tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Train sentencepiece tokenizer on data.txt\n",
    "spm.SentencePieceTrainer.train(input='data.txt', model_prefix='spm', vocab_size=10000)\n",
    "\n",
    "# Load the trained sentencepiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm.model')\n",
    "\n",
    "# Replace XLMR's tokenizer with the new tokenizer\n",
    "tokenizer.spm_model = sp\n",
    "\n",
    "# Freeze all parameters except for the embedding layer\n",
    "for name, param in model.named_parameters():\n",
    "    if 'embeddings' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "# Train a new embedding layer with the new tokenizer\n",
    "config = XLMRobertaConfig.from_pretrained('xlm-roberta-base')\n",
    "config.vocab_size = len(tokenizer)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the model on your task\n",
    "# ... (your code for training the model)\n",
    "\n",
    "# Save the new tokenizer and model\n",
    "tokenizer.save_pretrained('new_tokenizer')\n",
    "model.save_pretrained('new_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppls2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a41be343a2ce3b3345e21e42799e8fa6a247919c34d6f812341fb49cec4e6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
